{
  "name": "FCN",
  "tagline": "Fully Convolutional Network",
  "body": "## Fully Convolutional Network\r\nThis post implements a Fully Convolutional Network, inspired after [Springenberg's article on ArXiv in 2015](http://arxiv.org/abs/1412.6806). In many ways, the implementation is similar to my earlier [CNN for MNIST](http://robromijnders.github.io/tensorflow_basic/). The main difference is at testtime, where we feed images of another size than during training.\r\n\r\n## Fully convolutional\r\nMy interest for Fully Convolutional networks is the invariance for image sizes. Many healthcare applications of CNN's focus on focal pathologies, like tumors or fibroses. In the application for digital pathology, the size of the tumor might be ten by ten pixels, whereas the full image can be over thousand by thousand pixels. Therefore, we train CNN's with tiles of the bigger image. At testtime, though, we want to evaluate the full image and return a *heat-map* over the image where a possible patholohy might reside. \r\n\r\n## Setup\r\nTo simulate a situation where a local object of interest is immersed in a global piece of data, we *hide* MNIST handwritten images in larger images. The MATLAB-code *MNIST_generate_random_offset.m* does that. Next, we re-use code for a previous MNIST project to train a CNN. This CNN maps 28 by 28 pixel images to a distribution over the ten labels. The essential difference in our graph versus the previous implementation is the reshaping part:\r\n```python\r\n  x_image = tf.cond(test_large, lambda: tf.reshape(x, [-1,110,110,1]), lambda: tf.reshape(x, [-1,28,28,1]))\r\n```\r\nThe conditional reshapes the input vector into 28 by 28 or 110 by 100, depending on the fed boolean Tensor, test_large.\r\nIn Tensorboard, we see this\r\n![graph_reshape](https://github.com/RobRomijnders/FCN/blob/master/graph.png?raw=true) \r\n\r\n## Output\r\nThe Softmax output layer of the CNN calculates a distribution over the target labels. At train-time, we train on the cross-entropy of this distribution. At test-time, though, the fed images are 110 by 110. Consequently, the final layer has activation grids, rather than activation vectors. Instead of feeding this to a Softmax layer, we plot these activations as heat-maps. Softmax is a monotonically increasing function. Hence, high values in these heat-maps correspond to high probabilities of the activated area being a particular label.\r\n\r\n## Results\r\nA few examples of the resulting heat-maps:\r\n![Example1](https://github.com/RobRomijnders/FCN/blob/master/images/sample_1_2.png?raw=true)\r\n![Example2](https://github.com/RobRomijnders/FCN/blob/master/images/sample_2_1.png?raw=true)\r\n![Example3](https://github.com/RobRomijnders/FCN/blob/master/images/sample_3_5.png?raw=true)\r\n![Example4](https://github.com/RobRomijnders/FCN/blob/master/images/sample_4_9.png?raw=true)\r\n![Example5](https://github.com/RobRomijnders/FCN/blob/master/images/sample_5_9.png?raw=true)\r\n![Example6](https://github.com/RobRomijnders/FCN/blob/master/images/sample_6_2.png?raw=true)\r\n![Example7](https://github.com/RobRomijnders/FCN/blob/master/images/sample_7_5.png?raw=true)\r\n![Example8](https://github.com/RobRomijnders/FCN/blob/master/images/sample_8_6.png?raw=true)\r\n![Example9](https://github.com/RobRomijnders/FCN/blob/master/images/sample_9_1.png?raw=true)\r\n\r\nAs always, I am curious to any comments and questions. Reach me at romijndersrob@gmail.com\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}